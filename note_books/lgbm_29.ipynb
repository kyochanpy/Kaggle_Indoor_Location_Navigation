{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled16.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kyochanpy/Kaggle_Indoor_Location_Navigation/blob/main/note_books/lgbm_29.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SU575qxxCew"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import shutil\n",
        "import scipy.stats as stats\n",
        "from pathlib import Path\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "import lightgbm as lgb\n",
        "\n",
        "import psutil\n",
        "import random\n",
        "import os\n",
        "import time\n",
        "import sys\n",
        "import math\n",
        "from contextlib import contextmanager\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGrb1gPntbpG",
        "outputId": "1039ec64-dd14-487a-e213-c04409a1093f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sm4BoEzDxFPe"
      },
      "source": [
        "N_SPLITS = 10\n",
        "SEED = 42\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWuzNzX5xN-s"
      },
      "source": [
        "LOG_PATH = Path(\"./log/\")\n",
        "LOG_PATH.mkdir(parents=True, exist_ok=True)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuXjg4fnxQsS"
      },
      "source": [
        "@contextmanager\n",
        "def timer(name: str):\n",
        "    t0 = time.time()\n",
        "    p = psutil.Process(os.getpid())\n",
        "    m0 = p.memory_info()[0] / 2. ** 30\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        m1 = p.memory_info()[0] / 2. ** 30\n",
        "        delta = m1 - m0\n",
        "        sign = '+' if delta >= 0 else '-'\n",
        "        delta = math.fabs(delta)\n",
        "        print(f\"[{m1:.1f}GB({sign}{delta:.1f}GB): {time.time() - t0:.3f}sec] {name}\", file=sys.stderr)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-WJLWe9xUWC"
      },
      "source": [
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    \n",
        "def comp_metric(xhat, yhat, x, y):\n",
        "    intermediate = np.sqrt(np.power(xhat-x, 2) + np.power(yhat-y, 2))\n",
        "    return intermediate.sum()/xhat.shape[0]\n",
        "\n",
        "\n",
        "def score_log(df: pd.DataFrame, num_files: int, nam_file: str, data_shape: tuple, n_fold: int, seed: int, mpe: float):\n",
        "    score_dict = {'n_files': num_files, 'file_name': nam_file, 'shape': data_shape, 'fold': n_fold, 'seed': seed, 'score': mpe}\n",
        "    # noinspection PyTypeChecker\n",
        "    df = pd.concat([df, pd.DataFrame.from_dict([score_dict])])\n",
        "    df.to_csv(LOG_PATH / f\"log_score.csv\", index=False)\n",
        "    return df"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Y0snFNmxYSg"
      },
      "source": [
        "set_seed(SEED)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63hV5hB1xaad"
      },
      "source": [
        "feature_dir = \"/content/drive/MyDrive/all_data\"\n",
        "train_files = sorted(glob.glob(os.path.join(feature_dir, '*_train.csv')))\n",
        "test_files = sorted(glob.glob(os.path.join(feature_dir, '*_test.csv')))\n",
        "subm = pd.read_csv('/content/drive/MyDrive/sample_submission.csv', index_col=0)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrCKhgNdxxc4"
      },
      "source": [
        "lgb_params = {'objective': 'root_mean_squared_error',\n",
        "              'boosting_type': 'gbdt',\n",
        "              'n_estimators': 50000,\n",
        "              'learning_rate': 0.1,\n",
        "              'num_leaves': 90,\n",
        "              'colsample_bytree': 0.4,\n",
        "              'subsample': 0.6,\n",
        "              'subsample_freq': 2,\n",
        "              'bagging_seed': SEED,\n",
        "              'reg_alpha': 8,\n",
        "              'reg_lambda': 2,\n",
        "              'random_state': SEED,\n",
        "              'n_jobs': -1\n",
        "              }"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKznA-EnAvv3"
      },
      "source": [
        "# 共通関数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPLBtNI7x11v"
      },
      "source": [
        "def get_beacon_features(input_df):\n",
        "    output_df = input_df.loc[:, ['beacon_rssi', 'beacon_distance']]\n",
        "    return output_df\n",
        "\n",
        "def get_mag_features(input_df):\n",
        "    output_df = input_df.loc[:, ['mag_y', 'mag_z']]\n",
        "    return output_df\n",
        "\n",
        "def get_le_macaddr_features(input_df):\n",
        "    output_df = pd.DataFrame()\n",
        "    le = LabelEncoder()\n",
        "    le.fit(input_df['beacon_mac_addr'].astype(str))\n",
        "    output_df[\"LE_beacon_mac_addr\"] = le.transform(input_df['beacon_mac_addr'].astype(str))\n",
        "    return output_df\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLu9HDMctvgT"
      },
      "source": [
        "def get_fixed_rotation_vector(input_df):\n",
        "    input_df_ = input_df.loc[:, ['rotation_x', 'rotation_y', 'rotation_z']]\n",
        "    x_list = []\n",
        "    y_list = []\n",
        "    z_list = []\n",
        "    for x in input_df_['rotation_x'].values:\n",
        "        if x <= -0.5:\n",
        "            x = x + 0.5\n",
        "        elif x >= 0.5:\n",
        "            x = x - 0.5\n",
        "        x_list.append(x)\n",
        "    for y in input_df_['rotation_y'].values:\n",
        "        if y <= 0:\n",
        "            y = y + 1\n",
        "        y_list.append(y)\n",
        "    for z in input_df_['rotation_z'].values:\n",
        "        if z <= 0:\n",
        "            z = z + 1\n",
        "        z_list.append(z)\n",
        "    output_df = pd.DataFrame()\n",
        "    output_df['fixed_rotation_x'] = x_list\n",
        "    output_df['fixed_rotation_y'] = y_list\n",
        "    output_df['fixed_rotation_z'] = z_list\n",
        "    return output_df\n",
        "    \n",
        "def get_deg_rotation_vector(input_df):\n",
        "    input_df_ = input_df.loc[:, ['rotation_x', 'rotation_y', 'rotation_z']]\n",
        "    x_list = []\n",
        "    y_list = []\n",
        "    z_list = []\n",
        "    for x in input_df_['rotation_x'].values:\n",
        "        x_ = math.degrees(math.asin(x))\n",
        "        if x_ <= -45:\n",
        "            x_ = x_ + 45\n",
        "        elif x_ >= 45:\n",
        "            x_ = x_ -45\n",
        "        x_list.append(x_)\n",
        "    for y in input_df_['rotation_y'].values:\n",
        "        y_ = math.degrees(math.asin(y))\n",
        "        if y_ <= 0:\n",
        "            y_ = y_ + 90\n",
        "        y_list.append(y_)\n",
        "    for z in input_df_['rotation_z'].values:\n",
        "        z_ = math.degrees(math.asin(z))\n",
        "        if z_ <= 0:\n",
        "            z_ = z_ + 90\n",
        "        z_list.append(z_)\n",
        "    output_df = pd.DataFrame()\n",
        "    output_df['deg_rotation_x'] = x_list\n",
        "    output_df['deg_rotation_y'] = y_list\n",
        "    output_df['deg_rotation_z'] = z_list\n",
        "    return output_df\n",
        "\n",
        "def get_mag_rotation_features(input_df):\n",
        "    input_df_ = pd.concat([input_df, get_fixed_rotation_vector(input_df), get_deg_rotation_vector(input_df)], axis=1)\n",
        "    output_df = pd.DataFrame()\n",
        "    n_list = []\n",
        "    input_df_['fixed_deg_rotation_zz'] = input_df_['fixed_rotation_z'] * input_df_['deg_rotation_z']\n",
        "    input_df_['mag_rotation_zz'] = input_df_['mag_z'] * input_df_['fixed_deg_rotation_zz'] * 0.05\n",
        "    output_df['ud_mag_rotation_zz'] = abs(input_df_['mag_rotation_zz'].values - input_df_['mag_rotation_zz'].max())\n",
        "    return output_df\n",
        "\n",
        "def get_distance_features(input_df):\n",
        "    input_df_ = pd.concat([input_df, get_mag_rotation_features(input_df)], axis=1)\n",
        "    output_df = pd.DataFrame()\n",
        "    output_df['work_distance_y'] = abs(input_df_['acc_y'] * input_df_['ud_mag_rotation_zz'] * input_df_['mag_z'] * 0.01)\n",
        "    output_df['work_distance_z'] = abs(input_df_['acc_z'] * input_df_['ud_mag_rotation_zz'] * input_df_['mag_z'] * 0.01)\n",
        "    output_df['work_distance_yz'] = abs(output_df['work_distance_y'] + output_df['work_distance_z']) * 0.2\n",
        "    return output_df\n",
        "\n",
        "def get_sum_sensor_features(input_df):\n",
        "    output_df = pd.DataFrame()\n",
        "    output_df['sum_sensor_x'] = input_df['acc_x'] + input_df['acc_unc_x'] + input_df['acc_unc_x2'] + input_df['gyro_x'] + input_df['gyro_unc_x'] + input_df['gyro_unc_x2'] + input_df['mag_x'] + input_df['mag_unc_x'] + input_df['mag_unc_x2'] + input_df['rotation_x']\n",
        "    output_df['sum_sensor_y'] = input_df['acc_y'] + input_df['acc_unc_y'] + input_df['acc_unc_y2'] + input_df['gyro_y'] + input_df['gyro_unc_y'] + input_df['gyro_unc_y2'] + input_df['mag_y'] + input_df['mag_unc_y'] + input_df['mag_unc_y2'] + input_df['rotation_y']\n",
        "    output_df['sum_sensor_xy'] = output_df['sum_sensor_x'] + output_df['sum_sensor_y']\n",
        "    return output_df\n",
        "\n",
        "def get_fixed_sum_features(input_df):\n",
        "    input_df_ = pd.DataFrame()\n",
        "    output_df = pd.DataFrame()\n",
        "    input_df_['sum_sensor_y'] = input_df['acc_y'] + input_df['acc_unc_y'] + input_df['gyro_y'] + input_df['gyro_unc_y'] + input_df['mag_y'] + input_df['mag_unc_y'] + input_df['rotation_y']\n",
        "    input_df_['sum_sensor_x'] = input_df['acc_x'] + input_df['acc_unc_x'] + input_df['gyro_x'] + input_df['gyro_unc_x'] + input_df['mag_x'] + input_df['mag_unc_x'] + input_df['rotation_x']\n",
        "    input_df_['sum_sensor_xy'] = input_df_['sum_sensor_x'] + input_df_['sum_sensor_y']\n",
        "    x_list = []\n",
        "    x_25 = input_df_['sum_sensor_x'].quantile(q=0.25)\n",
        "    y_list = []\n",
        "    y_25 = input_df_['sum_sensor_y'].quantile(q=0.25)\n",
        "    xy_list = []\n",
        "    xy_25 = input_df_['sum_sensor_xy'].quantile(q=0.25)\n",
        "    for x in input_df_['sum_sensor_x'].values:\n",
        "        x_ = abs(x - x_25) + abs(x_25)\n",
        "        x_list.append(x_)\n",
        "    for y in input_df_['sum_sensor_y'].values:\n",
        "        y_ = abs(y - y_25) + abs(y_25)\n",
        "        y_list.append(y_)\n",
        "    for xy in input_df_['sum_sensor_xy'].values:\n",
        "        xy_ = abs(xy - xy_25) + abs(xy_25)\n",
        "        xy_list.append(xy_)\n",
        "    output_df['fixed_sum_sensor_x'] = x_list\n",
        "    output_df['fixed_sum_sensor_y'] = y_list\n",
        "    output_df['fixed_sum_sensor_xy'] = xy_list\n",
        "    return output_df"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSH6WdJtxny6"
      },
      "source": [
        "# train用関数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OClZCWHA09X"
      },
      "source": [
        "def get_target_train(input_df):\n",
        "    output_df = input_df.loc[:, ['x', 'y', 'path', 'timestamp' ]]\n",
        "    return output_df\n",
        "\n",
        "def get_wifi_train_festures(input_df):\n",
        "    output_df = input_df.iloc[:, 1:-42]\n",
        "    return output_df"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xFMVu-nxrFe"
      },
      "source": [
        "# test用関数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoJ9DRYmDkpr"
      },
      "source": [
        "def get_target_test(input_df):\n",
        "    output_df = input_df.loc[:, ['site_path_timestamp', 'path', 'timestamp']]\n",
        "    return output_df\n",
        "\n",
        "def get_wifi_test_festures(input_df):\n",
        "    output_df = input_df.iloc[:, 1:-41]\n",
        "    return output_df\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wyzRY0mEq5H"
      },
      "source": [
        "def get_process_funcs_train():\n",
        "    funcs = [get_wifi_train_festures,\n",
        "             get_beacon_features,\n",
        "             get_le_macaddr_features,\n",
        "             get_mag_rotation_features,\n",
        "             get_mag_features,\n",
        "             get_fixed_rotation_vector,\n",
        "             get_deg_rotation_vector,\n",
        "             get_distance_features,\n",
        "             get_sum_sensor_features,\n",
        "             get_fixed_sum_features,\n",
        "             get_target_train]\n",
        "    return funcs\n",
        "\n",
        "def get_process_funcs_test():\n",
        "    funcs = [get_wifi_test_festures,\n",
        "             get_beacon_features,\n",
        "             get_le_macaddr_features,\n",
        "             get_mag_rotation_features,\n",
        "             get_mag_features,\n",
        "             get_fixed_rotation_vector,\n",
        "             get_deg_rotation_vector,\n",
        "             get_distance_features,\n",
        "             get_sum_sensor_features,\n",
        "             get_fixed_sum_features,\n",
        "             get_target_test]\n",
        "    return funcs\n",
        "\n",
        "def to_feature(input_df, funcs):\n",
        "    output_df = pd.DataFrame()\n",
        "    for func in tqdm(funcs, total=len(funcs)):\n",
        "        _df = func(input_df)\n",
        "        assert len(_df) == len(input_df), func.__name__\n",
        "        output_df = pd.concat([output_df, _df], axis=1)\n",
        "    return output_df"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZOk9Sagx1Do",
        "outputId": "50ddaa24-184e-4fee-dbf7-64249d0352b2"
      },
      "source": [
        "score_df = pd.DataFrame()\n",
        "oof = list()\n",
        "predictions = list()\n",
        "for n_files, file in enumerate(train_files):\n",
        "    data = pd.read_csv(file, index_col=0).rename({'f':'floor'}, axis=1).fillna(0)\n",
        "    test_data = pd.read_csv(test_files[n_files], index_col=0).fillna(0)\n",
        "\n",
        "    #process_funcs_train = get_process_funcs_train()\n",
        "    #process_funcs_test = get_process_funcs_test()\n",
        "    data = to_feature(data, get_process_funcs_train())\n",
        "    test_data = to_feature(test_data, get_process_funcs_test())\n",
        "\n",
        "    print(data.shape)\n",
        "    print(test_data.shape)\n",
        "\n",
        "    oof_x, oof_y = np.zeros(data.shape[0]), np.zeros(data.shape[0])\n",
        "    preds_x, preds_y = 0, 0\n",
        "\n",
        "    kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
        "    for fold, (trn_idx, val_idx) in enumerate(kf.split(data.iloc[:, :-4])):\n",
        "        X_train = data.iloc[trn_idx, :-4]\n",
        "        y_trainx = data.iloc[trn_idx, -4]\n",
        "        y_trainy = data.iloc[trn_idx, -3]\n",
        "\n",
        "        X_valid = data.iloc[val_idx, :-4]\n",
        "        y_validx = data.iloc[val_idx, -4]\n",
        "        y_validy = data.iloc[val_idx, -3]\n",
        "        \n",
        "        modelx = lgb.LGBMRegressor(**lgb_params)\n",
        "        with timer(\"fit X\"):\n",
        "            modelx.fit(X_train, y_trainx,\n",
        "                       eval_set=[(X_valid, y_validx)],\n",
        "                       eval_metric='rmse',\n",
        "                       verbose=False,\n",
        "                       early_stopping_rounds=20\n",
        "                       )\n",
        "\n",
        "        modely = lgb.LGBMRegressor(**lgb_params)\n",
        "        with timer(\"fit Y\"):\n",
        "            modely.fit(X_train, y_trainy,\n",
        "                       eval_set=[(X_valid, y_validy)],\n",
        "                       eval_metric='rmse',\n",
        "                       verbose=False,\n",
        "                       early_stopping_rounds=20\n",
        "                       )\n",
        "\n",
        "            \n",
        "        oof_x[val_idx] = modelx.predict(X_valid)\n",
        "        oof_y[val_idx] = modely.predict(X_valid)\n",
        "\n",
        "        preds_x += modelx.predict(test_data.iloc[:, :-3]) / N_SPLITS\n",
        "        preds_y += modely.predict(test_data.iloc[:, :-3]) / N_SPLITS\n",
        "        preds_f = test_data['floor'].values\n",
        "\n",
        "        score = comp_metric(oof_x[val_idx], oof_y[val_idx], y_validx.to_numpy(), y_validy.to_numpy())\n",
        "        print(f\"fold {fold}: mean position error {score}\")\n",
        "        score_df = score_log(score_df, n_files, os.path.basename(file), data.shape, fold, SEED, score)\n",
        "\n",
        "    print(\"*+\"*40)\n",
        "    print(f\"file #{n_files}, shape={data.shape}, name={os.path.basename(file)}\")\n",
        "    score = comp_metric(oof_x, oof_y,\n",
        "                        data.iloc[:, -4].to_numpy(), data.iloc[:, -3].to_numpy())\n",
        "    oof.append(score)\n",
        "    print(f\"mean position error {score}\")\n",
        "    print(\"*+\"*40)\n",
        "    score_df = score_log(score_df, n_files, os.path.basename(file), data.shape, 999, SEED, score)\n",
        "    \n",
        "    test_preds = pd.DataFrame(np.stack((preds_f, preds_x, preds_y))).T\n",
        "    test_preds.columns = subm.columns\n",
        "    test_preds.index = test_data[\"site_path_timestamp\"]\n",
        "    test_preds[\"floor\"] = test_preds[\"floor\"].astype(int)\n",
        "    predictions.append(test_preds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  9%|▉         | 1/11 [00:00<00:01,  9.83it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYcBPhblV8r-"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxdmRtRDdOid"
      },
      "source": [
        "all_preds = pd.concat(predictions)\n",
        "all_preds = all_preds.reindex(subm.index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYF3AwsoOUCy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "outputId": "3de4ad28-f673-4f07-cd74-dae022a5c258"
      },
      "source": [
        "all_preds"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>floor</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>site_path_timestamp</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5a0546857ecc773753327266_046cfa46be49fc10834815c6_0000000000009</th>\n",
              "      <td>0.0</td>\n",
              "      <td>93.958514</td>\n",
              "      <td>99.233139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5a0546857ecc773753327266_046cfa46be49fc10834815c6_0000000009017</th>\n",
              "      <td>0.0</td>\n",
              "      <td>91.940152</td>\n",
              "      <td>102.341637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5a0546857ecc773753327266_046cfa46be49fc10834815c6_0000000015326</th>\n",
              "      <td>0.0</td>\n",
              "      <td>89.112067</td>\n",
              "      <td>105.326688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5a0546857ecc773753327266_046cfa46be49fc10834815c6_0000000018763</th>\n",
              "      <td>0.0</td>\n",
              "      <td>89.678749</td>\n",
              "      <td>105.679917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5a0546857ecc773753327266_046cfa46be49fc10834815c6_0000000022328</th>\n",
              "      <td>0.0</td>\n",
              "      <td>90.620852</td>\n",
              "      <td>110.508695</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5dc8cea7659e181adb076a3f_fd64de8c4a2fc5ebb0e9f412_0000000082589</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5dc8cea7659e181adb076a3f_fd64de8c4a2fc5ebb0e9f412_0000000085758</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5dc8cea7659e181adb076a3f_fd64de8c4a2fc5ebb0e9f412_0000000090895</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5dc8cea7659e181adb076a3f_fd64de8c4a2fc5ebb0e9f412_0000000096899</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5dc8cea7659e181adb076a3f_fd64de8c4a2fc5ebb0e9f412_0000000100447</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10133 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    floor  ...           y\n",
              "site_path_timestamp                                        ...            \n",
              "5a0546857ecc773753327266_046cfa46be49fc10834815...    0.0  ...   99.233139\n",
              "5a0546857ecc773753327266_046cfa46be49fc10834815...    0.0  ...  102.341637\n",
              "5a0546857ecc773753327266_046cfa46be49fc10834815...    0.0  ...  105.326688\n",
              "5a0546857ecc773753327266_046cfa46be49fc10834815...    0.0  ...  105.679917\n",
              "5a0546857ecc773753327266_046cfa46be49fc10834815...    0.0  ...  110.508695\n",
              "...                                                   ...  ...         ...\n",
              "5dc8cea7659e181adb076a3f_fd64de8c4a2fc5ebb0e9f4...    NaN  ...         NaN\n",
              "5dc8cea7659e181adb076a3f_fd64de8c4a2fc5ebb0e9f4...    NaN  ...         NaN\n",
              "5dc8cea7659e181adb076a3f_fd64de8c4a2fc5ebb0e9f4...    NaN  ...         NaN\n",
              "5dc8cea7659e181adb076a3f_fd64de8c4a2fc5ebb0e9f4...    NaN  ...         NaN\n",
              "5dc8cea7659e181adb076a3f_fd64de8c4a2fc5ebb0e9f4...    NaN  ...         NaN\n",
              "\n",
              "[10133 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuIro-hLyGS9"
      },
      "source": [
        "all_preds = pd.concat(predictions)\n",
        "all_preds = all_preds.reindex(subm.index)\n",
        "all_preds.to_csv('submission_lgbm_28.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwcjO7RAyjlL"
      },
      "source": [
        "!mv /content/submission_lgbm_28.csv /content/drive/MyDrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKx4AojKgO-a"
      },
      "source": [
        "floor = pd.read_csv('/content/drive/MyDrive/only_accurate_floor.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVYZsTrLgMPK"
      },
      "source": [
        "all_preds['floor'] = floor['floor'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iObJE_ugabC"
      },
      "source": [
        "all_preds.to_csv('submission_lgbm_27.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bLtT9APgZsf"
      },
      "source": [
        "!mv /content/submission_lgbm_27.csv /content/drive/MyDrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spMV_r8tym-p"
      },
      "source": [
        "pd."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}